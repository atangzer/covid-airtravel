{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build NA covid data for all airport comparisons\n",
    "# Build total + new NA cases\n",
    "NaData = pd.read_csv(\"Archieved-Data/casesNA.csv\", compression = 'gzip')\n",
    "NaData[\"Date\"] = pd.to_datetime(NaData[\"Date\"])\n",
    "NaSums = NaData.groupby(NaData['Date']).sum().reset_index()\n",
    "naCases = NaSums[['Date','Confirmed']]\n",
    "temp = naCases.shift(1)\n",
    "naCases['New'] = naCases['Confirmed'] - temp['Confirmed']\n",
    "naCases.loc[0,\"New\"] = 0\n",
    "new_cases_smooth = lowess(naCases['New'], naCases[\"Date\"], frac =0.05)\n",
    "confirm_smooth = lowess(naCases['Confirmed'], naCases[\"Date\"], frac =0.05)\n",
    "naCases['New_smooth'] = new_cases_smooth[:,1]\n",
    "naCases['Confirm_smooth'] = confirm_smooth[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvr = pd.read_csv('yvr4analyze_both.csv')\n",
    "yvrCases = pd.read_csv('Covid-Data/yvr-modified.csv')\n",
    "yvr[\"Date\"] = pd.to_datetime(yvr[\"Date\"])\n",
    "yvrCases[\"Date\"] = pd.to_datetime(yvrCases[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.plot(yvr[\"Date\"], yvr['PercentOfBaseline'], 'b-', label = 'Percentage Baseline')\n",
    "plt.plot(yvrCases[\"Date\"], yvrCases['Difference'], 'r-', label = 'Difference in Cases', alpha = 0.4)\n",
    "#plt.plot(yvrCases[\"Date\"], yvrCases['Confirmed'], 'm-', label = 'Difference in Cases', alpha = 0.4)\n",
    "plt.xlabel(\"Mid-March to Mid October\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that new cases per day is 0 sometimes, which isn't actually true, just that maybe\n",
    "# the cases found for a couple of days aren't logged that day, but instead logged on another day, so we should\n",
    "# smooth the new cases over a couple of days\n",
    "# We also want to smooth out the weekly cycles in the data to see the long-run correlations\n",
    "new_cases_smooth = lowess(yvrCases['Difference'], yvrCases[\"timestamp\"], frac =0.05)#,is_sorted = True, return_sorted=False)\n",
    "baseline_smooth = lowess(yvr['PercentOfBaseline'], yvr[\"Date\"], frac =0.04) #to reduce weekly cycle effect 7/214 = 0.327\n",
    "yvrCases['Diff_smooth'] = new_cases_smooth[:,1]\n",
    "yvr['Baseline_smooth'] = baseline_smooth[:,1]\n",
    "# This is to make sure both yvr and yvrCases start and end on the same dates\n",
    "if (yvr.count().loc['Date'] != yvrCases.count().loc['Date']):\n",
    "    joined = yvrCases[[\"Date\",\"Difference\",\"Confirmed\",\"Diff_smooth\"]].join(yvr[[\"Date\",\"PercentOfBaseline\",\"Baseline_smooth\"]].set_index('Date'), on='Date')\n",
    "    joined = joined.dropna()\n",
    "    X = joined[\"Diff_smooth\"]\n",
    "    y = joined[\"Baseline_smooth\"]\n",
    "else:\n",
    "    X = yvrCases[\"Diff_smooth\"]\n",
    "    y = yvr[\"Baseline_smooth\"]\n",
    "\n",
    "#print(joined.count())\n",
    "#can use X,y or original, doesn't really matter\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.plot(yvr[\"Date\"], yvr['PercentOfBaseline'], 'b.', label = 'Percentage Baseline', alpha=0.4)\n",
    "plt.plot(yvr[\"Date\"], yvr['Baseline_smooth'],'b-',label = 'Smoothed % Baseline')\n",
    "plt.plot(yvrCases[\"Date\"], yvrCases[\"Diff_smooth\"], 'r-', label = 'Smoothed New Cases', alpha = 0.5)\n",
    "plt.plot(yvrCases[\"Date\"], yvrCases['Difference'], 'r.', label = 'New Cases', alpha = 0.4)\n",
    "plt.xlabel(\"Mid-March to Mid October\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there statistical correlation between the two values- Baseline, and New Cases?\n",
    "# We use smoothed versions of data because don't want to analyze the weekly noise (tho holidays still exist)\n",
    "# y = Diff\n",
    "plt.plot(X,y, 'b.')\n",
    "plt.xlabel('New Cases per day')\n",
    "plt.ylabel('Baseline % of Airport Traffic')\n",
    "regression = stats.linregress(X, y)\n",
    "print(regression.rvalue)\n",
    "linearX = np.linspace(0,160)\n",
    "plt.plot(linearX, regression.slope*linearX + regression.intercept,'r-')\n",
    "#Data, especially X is unbalanced really. The r-value is too low to indicate some sort of relation between\n",
    "# the two pieces of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about confirmed?\n",
    "#smooth it, and then plot it\n",
    "confirm_smooth = lowess(joined['Confirmed'], joined[\"Date\"], frac =0.04) #to reduce weekly cycle effect 7/214 = 0.327\n",
    "confirmX = confirm_smooth[:,1]\n",
    "plt.plot(confirmX,y, 'b.')\n",
    "plt.xlabel('Total Confirmed Cases')\n",
    "plt.ylabel('Baseline % of Airport Traffic')\n",
    "reg = stats.linregress(confirmX, y)\n",
    "print(reg.rvalue)\n",
    "linearX = np.linspace(0,12000)\n",
    "plt.plot(linearX, reg.slope*linearX + reg.intercept,'r-')\n",
    "# slightly better but still terrible. Though interesting to see that consistent dip in airport traffic when the \n",
    "# total was around 2000 this was probably due to the inital shock in the early parts of the pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't seem like linear regression is what we want, let's try a polynomial\n",
    "# The data shows 2 dips --> matches with knowledge of the two waves\n",
    "plt.plot(confirmX,y, 'b.')\n",
    "plt.xlabel('Total Confirmed Cases')\n",
    "plt.ylabel('Baseline % of Airport Traffic')\n",
    "\n",
    "X = np.stack([confirmX], axis =1)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X,y)\n",
    "\n",
    "poly_regress = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PolynomialFeatures(degree=8, include_bias = True),\n",
    "    LinearRegression(fit_intercept = False))\n",
    "poly_regress.fit(X_train, y_train)\n",
    "\n",
    "linearX = np.stack([np.linspace(0,11500)],axis=1)\n",
    "\n",
    "plt.plot(linearX, poly_regress.predict(linearX), 'r-')\n",
    "print(poly_regress.score(X_train,y_train))\n",
    "print(poly_regress.score(X_valid,y_valid))\n",
    "# Oh hey we get something pretty good, but with poly regression, we can't exactly \n",
    "# predict into the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make sure both yvr and naCases start and end on the same dates\n",
    "if (yvr.count().loc['Date'] != naCases.count().loc['Date']):\n",
    "    joined = naCases[[\"Date\",\"Confirmed\",\"New\"]].join(yvr[[\"Date\",\"PercentOfBaseline\",\"Baseline_smooth\"]].set_index('Date'), on='Date')\n",
    "    joined = joined.dropna()\n",
    "\n",
    "plt.scatter(joined['Confirmed'],joined['Baseline_smooth'],c=joined['Date'],cmap='winter' )\n",
    "plt.xlabel(\"North American Confirmed Cases\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.ylabel(\"Baseline Smoothed\")\n",
    "\n",
    "X = np.stack([joined['Confirmed']], axis =1)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X,y)\n",
    "\n",
    "poly_regress = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PolynomialFeatures(degree=8, include_bias = True),\n",
    "    LinearRegression(fit_intercept = False))\n",
    "poly_regress.fit(X_train, y_train)\n",
    "\n",
    "linearX = np.stack([np.linspace(0,11500)],axis=1)\n",
    "\n",
    "plt.plot(linearX, poly_regress.predict(linearX), 'r-')\n",
    "print(poly_regress.score(X_train,y_train))\n",
    "print(poly_regress.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML with new cases, total cases, na cases: total + new, vs y= Baseline\n",
    "#naCases --> Confirmed, New, New_smooth, Confirm_smooth\n",
    "#joined --> Confirmed, Confirmed_smooth, Difference, Diff_smooth, Baseline_smooth, PercentOfBaseline\n",
    "\n",
    "joined[\"Confirmed_smooth\"] = confirmX\n",
    "\n",
    "# X --> na_confirm, na_new, bc_confirm, bc_new\n",
    "X = naCases[[\"Confirm_smooth\", \"New_smooth\", \"Date\"]].join(joined[[\"Confirmed_smooth\",\"Diff_smooth\",\"Date\"]].set_index('Date'), on ='Date')\n",
    "X = X.rename(columns={\"Confirm_smooth\": \"na_confirm\", \"New_smooth\":\"na_new\", \"Confirmed_smooth\":\"ab_confirm\", \"Diff_smooth\":\"bc_new\"})\n",
    "X = X[[\"na_confirm\",\"na_new\",\"bc_confirm\",\"bc_new\"]].dropna()\n",
    "\n",
    "# y --> joined['Baseline_smooth']\n",
    "y = joined['Baseline_smooth'].rename(columns={\"Baseline_smooth\":\"yegBaseline\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GB regressor does the best out of all of them bc diff factors are weighed differently\n",
    "# Also not that much data, so neural network doesn't really work\n",
    "gbmodel = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    GradientBoostingRegressor(n_estimators=50, max_depth=5)\n",
    "    )\n",
    "gbmodel.fit(X_train, y_train)\n",
    "\n",
    "#linearX = np.stack([np.linspace(0,11500)],axis=1)\n",
    "#plt.plot(linearX, poly_regress.predict(linearX), 'r-')\n",
    "#print(knmodel.score(X_train,y_train), knmodel.score(X_valid,y_valid))\n",
    "#print(rfmodel.score(X_train,y_train), rfmodel.score(X_valid,y_valid))\n",
    "#print(nnmodel.score(X_train,y_train), nnmodel.score(X_valid,y_valid))\n",
    "print(gbmodel.score(X_train,y_train), gbmodel.score(X_valid,y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
