{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build NA covid data for all airport comparisons\n",
    "# Build total + new NA cases\n",
    "NaData = pd.read_csv(\"Archieved-Data/casesNA.csv\", compression = 'gzip')\n",
    "NaData[\"Date\"] = pd.to_datetime(NaData[\"Date\"])\n",
    "NaSums = NaData.groupby(NaData['Date']).sum().reset_index()\n",
    "naCases = NaSums[['Date','Confirmed']]\n",
    "temp = naCases.shift(1)\n",
    "naCases['New'] = naCases['Confirmed'] - temp['Confirmed']\n",
    "naCases.loc[0,\"New\"] = 0\n",
    "new_cases_smooth = lowess(naCases['New'], naCases[\"Date\"], frac =0.05)\n",
    "confirm_smooth = lowess(naCases['Confirmed'], naCases[\"Date\"], frac =0.05)\n",
    "naCases['New_smooth'] = new_cases_smooth[:,1]\n",
    "naCases['Confirm_smooth'] = confirm_smooth[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yul = pd.read_csv('yul4analyze_both.csv',index_col =0, parse_dates = [1])\n",
    "yulCases = pd.read_csv('Covid-Data/yul-modified.csv', index_col =0, parse_dates = [5])\n",
    "\n",
    "yul[\"Date\"] = pd.to_datetime(yul[\"Date\"])\n",
    "yulCases[\"Date\"] = pd.to_datetime(yulCases[\"Date\"])\n",
    "\n",
    "yulCases.loc[0, \"Difference\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well thats a funky looking plot.\n",
    "\"\"\"plt.figure(figsize = (20,5))\n",
    "plt.plot(yul[\"Date\"], yul['PercentOfBaseline'], 'b-', label = 'Percentage Baseline')\n",
    "plt.plot(yulCases[\"Date\"], yulCases['Difference'], 'r-', label = 'Difference in Cases', alpha = 0.4)\n",
    "#plt.plot(yulCases[\"Date\"], yulCases['Confirmed'], 'm-', label = 'Difference in Cases', alpha = 0.4)\n",
    "plt.xlabel(\"Mid-March to Mid October\")\n",
    "plt.legend()\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline seems constant; except for the small dip in March\n",
    "# Find factors to attribute to the two curves - small bump in the summer?\n",
    "# The second one might be an instance to not increase flights to YUL\n",
    "# lots of 0s in the beginning.... averaging or imputing the missing values might have been a good idea\n",
    "\n",
    "new_cases_smooth = lowess(yulCases['Difference'], yulCases[\"timestamp\"], frac =0.05)#,is_sorted = True, return_sorted=False)\n",
    "baseline_smooth = lowess(yul['PercentOfBaseline'], yul[\"Date\"], frac =0.04) #to reduce weekly cycle effect 7/214 = 0.327\n",
    "yulCases['Diff_smooth'] = new_cases_smooth[:,1]\n",
    "yul['Baseline_smooth'] = baseline_smooth[:,1]\n",
    "# Ensure both yul and yulCases start and end on the same dates\n",
    "if (yul.count().loc['Date'] != yulCases.count().loc['Date']):\n",
    "    joined = yulCases[[\"Date\",\"Difference\",\"Confirmed\",\"Diff_smooth\"]].join(yul[[\"Date\",\"PercentOfBaseline\",\"Baseline_smooth\"]].set_index('Date'), on='Date')\n",
    "    joined = joined.dropna()\n",
    "    X = joined[\"Diff_smooth\"]\n",
    "    y = joined[\"Baseline_smooth\"]\n",
    "else:\n",
    "    X = yulCases[\"Diff_smooth\"]\n",
    "    y = yul[\"Baseline_smooth\"]\n",
    "\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.plot(yul[\"Date\"], yul['PercentOfBaseline'], 'b.', label = 'Percentage Baseline', alpha=0.4)\n",
    "plt.plot(yul[\"Date\"], yul['Baseline_smooth'],'b-',label = 'Smoothed % Baseline')\n",
    "plt.plot(yulCases[\"Date\"], yulCases[\"Diff_smooth\"], 'r-', label = 'Smoothed New Cases', alpha = 0.5)\n",
    "plt.plot(yulCases[\"Date\"], yulCases['Difference'], 'r.', label = 'New Cases', alpha = 0.4)\n",
    "plt.xlabel(\"Mid-March to Mid October\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find statistical correlation between the two values- Baseline, and New Cases, using the filtered data from above\n",
    "\n",
    "plt.plot(X,y, 'b.')\n",
    "plt.xlabel('New Cases per day')\n",
    "plt.ylabel('Baseline % of Airport Traffic')\n",
    "regression = stats.linregress(X, y)\n",
    "\n",
    "print(regression.rvalue)\n",
    "\n",
    "linearX = np.linspace(0, max(X)) \n",
    "plt.plot(linearX, regression.slope*linearX + regression.intercept,'r-')\n",
    "\n",
    "# neglible correlation. so assume none at all - look into possible factors\n",
    "# are people still travelling, or not travelling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for Number of Confirmed Patients\n",
    "confirm_smooth = lowess(joined['Confirmed'], joined[\"Date\"], frac =0.04)\n",
    "# to reduce weekly cycle effect 7/214 = 0.327\n",
    "confirmX = confirm_smooth[:,1]\n",
    "plt.plot(confirmX,y, 'b.')\n",
    "plt.xlabel('Total Confirmed Cases')\n",
    "plt.ylabel('Baseline % of Airport Traffic')\n",
    "\n",
    "reg = stats.linregress(confirmX, y)\n",
    "\n",
    "print(reg.rvalue)\n",
    "\n",
    "linearX = np.linspace(0, max(confirmX)) \n",
    "plt.plot(linearX, reg.slope*linearX + reg.intercept,'r-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([confirmX], axis =1)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Poly Regress\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=3, include_bias=True),\n",
    "    LinearRegression(fit_intercept=False)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "linearX = np.linspace(0,max(X_train)).reshape(-1, 1) #maybe change params\n",
    "plt.plot(linearX, model.predict(linearX), 'r-', label = \"Predicted Line\")\n",
    "plt.plot(X, y, 'b.', label = 'Presented Data')\n",
    "plt.legend()\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Analyzing statistical correlation in --> Wave 1 (Apr - June)\n",
    "first_wave = joined[joined['Date']<'2020-06-20']\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "\"\"\"plt.plot(first_wave[\"Date\"], first_wave['PercentOfBaseline'], 'b.', label = 'Percentage Baseline', alpha=0.7)\n",
    "plt.plot(first_wave[\"Date\"], first_wave['Baseline_smooth'],'b-',label = 'Smoothed % Baseline')\n",
    "plt.plot(first_wave[\"Date\"], first_wave[\"Diff_smooth\"], 'r-', label = 'Smoothed New Cases', alpha = 0.5)\n",
    "plt.plot(first_wave[\"Date\"], first_wave['Difference'], 'r.', label = 'New Cases', alpha = 0.7)\n",
    "plt.xlabel(\"Mid-March to Mid-May: First Wave\")\n",
    "plt.legend()\n",
    "plt.show() #two things over time\"\"\"\n",
    "\n",
    "plt.plot(first_wave[\"Confirmed\"],first_wave[\"Baseline_smooth\"], 'b.', label = 'Presented Data')\n",
    "plt.xlabel(\"Total Confirmed Cases in QC\")\n",
    "plt.ylabel(\"Baseline % of airport traffic\")\n",
    "\n",
    "# Entire Curve\n",
    "regression = stats.linregress(first_wave[\"Confirmed\"], first_wave[\"Baseline_smooth\"])\n",
    "print(regression.rvalue)\n",
    "# Moderate correlation\n",
    "linearX = np.linspace(400, max(first_wave[\"Confirmed\"]))\n",
    "plt.plot(linearX, regression.slope*linearX + regression.intercept,'r--', label = 'Entire Curve')\n",
    "\n",
    "# Analyze the tail of the first wave: May to mid June\n",
    "reg2 = stats.linregress(first_wave[\"Confirmed\"].tail(50), first_wave[\"Baseline_smooth\"].tail(50))\n",
    "print(reg2.rvalue)\n",
    "# Strong correlation\n",
    "lin2 = np.linspace(400, max(first_wave[\"Confirmed\"].tail(50)))\n",
    "plt.plot(lin2, reg2.slope*lin2+ reg2.intercept,'g--', label = 'End of Curve')\n",
    "\n",
    "# Analyze the steep climb during the rise (first 11 days):\n",
    "reg3 = stats.linregress(first_wave[\"Confirmed\"].head(30), first_wave[\"Baseline_smooth\"].head(30))\n",
    "print(reg3.rvalue)\n",
    "# Moderate correlation\n",
    "lin3 = np.linspace(400, max(first_wave[\"Confirmed\"].head(30)))\n",
    "plt.plot(lin3, reg3.slope*lin3+ reg3.intercept,'m--', label = 'Beginning of Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Scary how similar this is to AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wave 2: Sept - Oct (Ongoing)\n",
    "sec_wave = joined[joined['Date']<'2020-10-16']\n",
    "sec_wave = sec_wave[sec_wave['Date']>'2020-08-30']\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "\"\"\"plt.plot(sec_wave[\"Date\"], sec_wave['PercentOfBaseline'], 'b.', label = 'Percentage Baseline', alpha=0.7)\n",
    "plt.plot(sec_wave[\"Date\"], sec_wave['Baseline_smooth'],'b-',label = 'Smoothed % Baseline')\n",
    "plt.plot(sec_wave[\"Date\"], sec_wave[\"Diff_smooth\"], 'r-', label = 'Smoothed New Cases', alpha = 0.5)\n",
    "plt.plot(sec_wave[\"Date\"], sec_wave['Difference'], 'r.', label = 'New Cases', alpha = 0.7)\n",
    "plt.xlabel(\"Mid-March to Mid-May: sec Wave\")\n",
    "plt.legend()\n",
    "plt.show() #two things over time\"\"\"\n",
    "\n",
    "plt.plot(sec_wave[\"Confirmed\"],sec_wave[\"Baseline_smooth\"], 'b.', label = 'Presented Data')\n",
    "plt.xlabel(\"Total Confirmed Cases in QC\")\n",
    "plt.ylabel(\"Baseline % of airport traffic\")\n",
    "\n",
    "# Entire Curve\n",
    "regression = stats.linregress(sec_wave[\"Confirmed\"], sec_wave[\"Baseline_smooth\"])\n",
    "print(regression.rvalue)\n",
    "# No correlation\n",
    "linearX = np.linspace(400, max(sec_wave[\"Confirmed\"]))\n",
    "plt.plot(linearX, regression.slope*linearX + regression.intercept,'r--', label = 'Entire Curve')\n",
    "\n",
    "# Analyze the tail of the 2nd wave: \n",
    "reg2 = stats.linregress(sec_wave[\"Confirmed\"].tail(20), sec_wave[\"Baseline_smooth\"].tail(20))\n",
    "print(reg2.rvalue)\n",
    "# Weak correlation\n",
    "lin2 = np.linspace(400, max(sec_wave[\"Confirmed\"].tail(20)))\n",
    "plt.plot(lin2, reg2.slope*lin2+ reg2.intercept,'g--', label = 'End of Curve')\n",
    "\n",
    "# Analyze the climb (1 month):\n",
    "reg3 = stats.linregress(sec_wave[\"Confirmed\"].head(30), sec_wave[\"Baseline_smooth\"].head(30))\n",
    "print(reg3.rvalue)\n",
    "# Weak correlation\n",
    "lin3 = np.linspace(400, max(sec_wave[\"Confirmed\"].head(30)))\n",
    "plt.plot(lin3, reg3.slope*lin3+ reg3.intercept,'m--', label = 'Beginning of Curve')\n",
    "plt.legend()\n",
    "\n",
    "# This is kind of funky - what happened here?\n",
    "# Did people decide not to travel or something? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make sure both yul and naCases start and end on the same dates\n",
    "if (yul.count().loc['Date'] != naCases.count().loc['Date']):\n",
    "    na = naCases[[\"Date\",\"Confirmed\",\"New\"]].join(yul[[\"Date\",\"PercentOfBaseline\",\"Baseline_smooth\"]].set_index('Date'), on='Date')\n",
    "    na = na.dropna()\n",
    "    \n",
    "plt.scatter(na['Confirmed'],na['Baseline_smooth'],c=na['Date'],cmap='winter' )\n",
    "plt.xlabel(\"North American Confirmed Cases\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.ylabel(\"Baseline Smoothed\")\n",
    "\n",
    "X = np.stack([na['Confirmed']], axis =1)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X,np.stack([na['Baseline_smooth']], axis =1))\n",
    "\n",
    "poly_regress = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PolynomialFeatures(degree=14, include_bias = True),\n",
    "    LinearRegression(fit_intercept = False))\n",
    "poly_regress.fit(X_train, y_train)\n",
    "\n",
    "linearX = np.stack([np.linspace(0,8200000)],axis=1)\n",
    "\n",
    "plt.plot(linearX, poly_regress.predict(linearX), 'r-')\n",
    "print(poly_regress.score(X_train,y_train))\n",
    "print(poly_regress.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML with new cases, total cases, na cases: total + new, vs y= Baseline\n",
    "# naCases --> Confirmed, New, New_smooth, Confirm_smooth\n",
    "# joined --> Confirmed, Confirmed_smooth, Difference, Diff_smooth, Baseline_smooth, PercentOfBaseline\n",
    "\n",
    "joined[\"Confirmed_smooth\"] = confirmX\n",
    "joined\n",
    "# X --> na_confirm, na_new, ab_confirm, ab_new\n",
    "X = naCases[[\"Confirm_smooth\", \"New_smooth\", \"Date\"]].join(joined[[\"Confirmed_smooth\",\"Diff_smooth\",\"Date\"]].set_index('Date'), on ='Date')\n",
    "X = X.rename(columns={\"Confirm_smooth\": \"na_confirm\", \"New_smooth\":\"na_new\", \"Confirmed_smooth\":\"ab_confirm\", \"Diff_smooth\":\"ab_new\"})\n",
    "X = X[[\"na_confirm\",\"na_new\",\"ab_confirm\",\"ab_new\"]].dropna()\n",
    "\n",
    "# y --> joined['Baseline_smooth']\n",
    "y = joined['Baseline_smooth'].rename(columns={\"Baseline_smooth\":\"yegBaseline\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GB regressor does the best out of all of them bc diff factors are weighed differently\n",
    "# Also not that much data, so neural network doesn't really work\n",
    "gbmodel = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    GradientBoostingRegressor(n_estimators=50, max_depth=5)\n",
    "    )\n",
    "gbmodel.fit(X_train, y_train)\n",
    "\n",
    "print(gbmodel.score(X_train,y_train), gbmodel.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
